{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Machine Learning  \n",
    "\n",
    "## Assignment 6:  Preprocessing Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't learn technical subjects without hands-on practice. The assignments are an important part of the course. To submit this assignment you will need to make sure that you save your Jupyter notebook. \n",
    "\n",
    "Below are the links of 2 videos that explain:\n",
    "\n",
    "1. [How to save your Jupyter notebook](https://youtu.be/0aoLgBoAUSA) and,       \n",
    "2. [How to answer a question in a Jupyter notebook assignment](https://youtu.be/7j0WKhI3W4s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Learning Goals:\n",
    "\n",
    "By the end of the module, students are expected to:\n",
    "\n",
    "- Explain `handle_unknown=\"ignore\"` hyperparameter of `scikit-learn`'s `OneHotEncoder`.\n",
    "- Identify when it's appropriate to apply ordinal encoding vs one-hot encoding.\n",
    "- Explain strategies to deal with categorical variables with too many categories.\n",
    "- Explain why text data needs a different treatment than categorical variables.\n",
    "- Use `scikit-learn`'s `CountVectorizer` to encode text data.\n",
    "- Explain different hyperparameters of `CountVectorizer`.\n",
    "- Use `ColumnTransformer` to build all our transformations together into one object and use it with `scikit-learn` pipelines.\n",
    "\n",
    "This assignment covers [Module 6](https://ml-learn.mds.ubc.ca/en/module6) of the online course. You should complete this module before attempting this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any place you see `...`, you must fill in the function, variable, or data to complete the code. Substitute the `None` with your completed code and answers then proceed to run the cell!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the questions in this assignment will have hidden tests. This means that no feedback will be given as to the correctness of your solution. It will be left up to you to decide if your answer is sufficiently correct. These questions are worth 2 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b520ca500fe6daf2b0cff02d35c21b7",
     "grade": false,
     "grade_id": "cell-5e92ac69d9a55154",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries needed for this lab\n",
    "from hashlib import sha1\n",
    "\n",
    "import altair as alt\n",
    "import graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import make_column_transformer \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    Normalizer,\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    normalize,\n",
    "    scale)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import test_assignment6 as t\n",
    "#alt.renderers.enable('mimetype')\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducing and Exploring the dataset <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "\n",
    "In this lab you will be working with [the Olympics Games DataSet](https://www.kaggle.com/samruddhim/olympics-althlete-events-analysis).\n",
    "\n",
    "Our problem is to predict the medal type of each example. \n",
    " You can find more information on the dataset and features [here](https://www.kaggle.com/samruddhim/olympics-althlete-events-analysis).\n",
    "\n",
    "\n",
    "*Note that many popular datasets have sex as a feature where the possible values are male and female. This representation reflects how the data were collected and is not meant to imply that, for example, gender is binary.*\n",
    "\n",
    "\n",
    "The following starter code preprocesses the data to get rid of rows with `NaN` values in the target column `Medal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medal_df = pd.read_csv(\"data/athlete_events.csv\")\n",
    "medal_df = medal_df.dropna(subset=['Medal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1** <br> {points: 1}  \n",
    "\n",
    "In order to avoid violating the golden rule, before we do anything with the data, let's split it.\n",
    "\n",
    "Split the data into `train_df` (80%) and `test_df` (20%). \n",
    "\n",
    "Keep the target column (`Medal`) in the splits so that we can use it in EDA. \n",
    "\n",
    "Make sure to set `random_state=123` for grading purposes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "398bda2225addd994c000cca833e4d4d",
     "grade": false,
     "grade_id": "cell-c9fddbf1159f2bc2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = None, None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ee0d61921ef4f124d18adc95f3aa3c5",
     "grade": true,
     "grade_id": "cell-a197fcf02a34fb83",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_1(train_df,test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2** <br> {points: 1}  \n",
    "\n",
    "How many examples are there in our training data? \n",
    "\n",
    "Save your answer in an object named `training_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7966d9aff51f0c62e34f9be90a9f6a1",
     "grade": false,
     "grade_id": "cell-bcb1ca7ea65e52b7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "training_size = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "training_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2320bc84859e3b0ec8d80c912fcd074b",
     "grade": true,
     "grade_id": "cell-5d56653aad604c93",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_2(training_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3** <br> {points: 3}  \n",
    "\n",
    "Let's examine our `train_df` a bit. \n",
    "\n",
    "What is the youngest and oldest age of an athlete that won a medal in the Olympics?\n",
    "\n",
    "Save the results in objects `youngest_age` and `oldest_age`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dda730e2a74a84a475f51eeab12f139e",
     "grade": false,
     "grade_id": "cell-a6c61e97a6a7d6d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "youngest_age = None\n",
    "oldest_age = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca570ab4d4177f24cc83841fb2d99400",
     "grade": true,
     "grade_id": "cell-f7048261a24cea8a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check that the variable exists\n",
    "assert 'oldest_age' in globals(\n",
    "), \"Please make sure that your solution is named 'oldest_age'\"\n",
    "\n",
    "# This test has been intentionally hidden. It will be up to you to decide if your solution\n",
    "# is sufficiently good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "adc6c0fe14b44ceb2856c616ad1a95f1",
     "grade": true,
     "grade_id": "cell-cc1717e5b43201b0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_3_2(youngest_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.4** <br> {points: 1}  \n",
    "\n",
    "Look at the column dtypes using `.info()`.\n",
    "\n",
    "How many non numeric **features** are there? \n",
    "\n",
    "Save the results in an object named `num_cat_feats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2225da96b6ad5cc5ed5d1de70fd7ab9e",
     "grade": false,
     "grade_id": "cell-5c808e415e98b0be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "num_cat_feats = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7899a4c872fa082fab5b050a99a73e74",
     "grade": true,
     "grade_id": "cell-0d096bba9ce22ae7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_4(num_cat_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.5** <br> {points: 3}  \n",
    "\n",
    "Let's take a look at some of the columns and the categories within them. \n",
    "\n",
    "Use `.describe` to answer the following questions. Save the describe dataframe in an object named `describe_df`.  \n",
    "\n",
    "a) Which categorical feature has the most unique values? Save this in an object named `most_unique`. \n",
    "\n",
    "b) How many binary columns are there? Save this in an object named `binary_cols`. \n",
    "\n",
    "c) How many categorical features have missing values? Save this number in an object named `missing_cat`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5545b1671e79653dadbcd69d87a76495",
     "grade": false,
     "grade_id": "cell-898755125a62cf22",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "most_unique = None\n",
    "binary_cols = None\n",
    "missing_cat = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ea69b979e81b9f60207f04212bbb327",
     "grade": true,
     "grade_id": "cell-27a94573d4090c6d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_5_1(most_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ff333f4572634ebf68e8fdc58b14648",
     "grade": true,
     "grade_id": "cell-7e9bda74ff7409ff",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_5_2(binary_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3008edc4096a9c503f033df717e4f785",
     "grade": true,
     "grade_id": "cell-8df6a68ea298ab95",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_5_3(missing_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.6** <br> {points: 2}  \n",
    "\n",
    "Filter or groupby the `train_df` dataframe to answer the next question. \n",
    "\n",
    "Which `NOC` won the most medals? Save this in an object named `most_medals`. \n",
    "\n",
    "Which `NOC` won the most `Gold` medals? Save this in an object named `most_gold`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0fb5f5a4c6831fdfcf3443e7289415e",
     "grade": false,
     "grade_id": "cell-bde68c09819e15f0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "most_medals = None\n",
    "most_gold = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a87b2113bd4c9f848363f4e922995063",
     "grade": true,
     "grade_id": "cell-a9ee1795366bea98",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_6_1(most_medals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c715b3d340b05e35edfe3c455b120f7b",
     "grade": true,
     "grade_id": "cell-9c95b1a4262f61d8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_6_2(most_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to separate feature vectors from the targets.\n",
    "\n",
    "We are only going to use the folowing columns:\n",
    "\n",
    "- `Sex`\n",
    "- `Age`\n",
    "- `Height`\n",
    "- `Weight`\n",
    "- `NOC`\n",
    "- `Year`\n",
    "- `Season`\n",
    "- `City`\n",
    "- `Sport`\n",
    "\n",
    "\n",
    "and using `Medal` as the target column. \n",
    "\n",
    "We've created  `X_train`, `y_train`, `X_test`, `y_test` for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=['ID', 'Name', 'Team', 'Event','Medal', 'Games'])\n",
    "y_train = train_df['Medal']\n",
    "\n",
    "X_test = test_df.drop(columns=['ID', 'Name', 'Team', 'Event','Medal', 'Games'])\n",
    "y_test = test_df['Medal']\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing and building your pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1** <br> {points: 4}  \n",
    "\n",
    "Before you can start preprocessing our data, you need to identify the binary, categorical, ordinal and numeric columns in your `X_train` and build lists of each feature type. \n",
    "\n",
    "\n",
    "Save the column names in lists named  `numeric_feats`, `binary_feats`, `categorical_feats` and `ordinal_feat`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09ebcaabe87884abc686431c0b58444f",
     "grade": false,
     "grade_id": "cell-43b6a1d4d1641894",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "numeric_feats = None \n",
    "binary_feats = None \n",
    "categorical_feats = None \n",
    "ordinal_feat = None \n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c8cfa626fd296dcf63739fce9db3c68",
     "grade": true,
     "grade_id": "cell-a21f3a066962f23f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_1_1(numeric_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9583fa5136fb2d0829bf005350e97c34",
     "grade": true,
     "grade_id": "cell-005c8032543d159e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_1_2(binary_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a95a9495fb6699f046e179566f0a5195",
     "grade": true,
     "grade_id": "cell-b27a3dcda89adec6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_1_3(categorical_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b92c0b09a2b55f5efd570151d559672",
     "grade": true,
     "grade_id": "cell-b43fdcc99ff0a5fa",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_1_4(ordinal_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2** <br> {points: 1}  \n",
    "\n",
    "Ok let's start making our pipelines. Use `make_pipeline()` to make a pipeline for the numeric features called `numeric_transformer`. \n",
    "\n",
    "Use `SimpleImputation()` with `strategy=median`. For the second step make sure to use standardization with `StandardScaler()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "272038691a2b2d29665fa2b0c46dad20",
     "grade": false,
     "grade_id": "cell-83073da6c449aa14",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "numeric_transformer = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9536a6e75fbef1369554fac8024e876d",
     "grade": true,
     "grade_id": "cell-438e0937d545d5a5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_2(numeric_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3** <br> {points: 1}  \n",
    "\n",
    "Next, use `make_pipeline()` to make a pipeline for the categorical features called `categorical_transformer`. \n",
    "\n",
    "Use `SimpleImputation()` with `strategy=most_frequent`. \n",
    "\n",
    "Make sure to use the necessary one-hot encoding transformer with `dtype=int` and `handle_unknown=\"ignore\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f227940c6b8e034b8ae3435d96b5b871",
     "grade": false,
     "grade_id": "cell-6967c75a66f431f2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "categorical_transformer = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0aa653f9f46004ff00e315d1a65b368e",
     "grade": true,
     "grade_id": "cell-53acdb809e9dda5a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_3(categorical_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4** <br> {points: 1}  \n",
    "  \n",
    "Use `make_pipeline()` to make a pipeline for the binary features call `binary_transformer`. \n",
    "\n",
    "Use `SimpleImputation()` with `strategy=most_frequent`. \n",
    "\n",
    "Make sure to use the necessary one-hot encoding transformer with `dtype=int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e09717dce4dfb9f1eaebb06669831126",
     "grade": false,
     "grade_id": "cell-7dd0bd03ae4e6318",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "binary_transformer = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74e4a7008494df664a2acc6369c6caa0",
     "grade": true,
     "grade_id": "cell-25e9e58ac84b4a2d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_4(binary_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5** <br> {points: 1}  \n",
    "\n",
    "\n",
    "Define a column transformer using [`make_column_transformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html) called `preprocessor` for the numerical, categorical, and remainding feature types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2b610cd853ef87e0d60f4b4b1b0f8d9",
     "grade": false,
     "grade_id": "cell-28dacb5959d6211d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7974551bfb35bb12919b7227c713f2a3",
     "grade": true,
     "grade_id": "cell-ff142c4bdd7dc427",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_5(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1** <br> {points: 1}  \n",
    "\n",
    "It's important to build a dummy classifier to compare our model to. Make a `DummyClassifier` using `strategy=\"prior\"`. \n",
    "\n",
    "Carry out 5-fold cross validation on `X_train` and `y_train` using ` cross_validate()`. Don't forget to include the training score. \n",
    "\n",
    "Save the results in a dataframe named `dummy_scores`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08eedaafc6264175397d726976be33cf",
     "grade": false,
     "grade_id": "cell-f05c1643222cdae8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_scores = None \n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5d46cccd150ad0d0f98d55a7f455735",
     "grade": true,
     "grade_id": "cell-22c0fb890efc07ba",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_1(dummy_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2** <br> {points: 1}  \n",
    "\n",
    "\n",
    "Define a main pipeline called `main_pipe` that transforms all the different features and uses a `RandomForestClassifier` model using `random_state=77` and setting the hyperparameter `n_estimators` to 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d20b1a87f3f348f56e87b0369fef3079",
     "grade": false,
     "grade_id": "cell-0fa24b8f5c0ffebc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "main_pipe = None\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "302502060294ab62b693527e287d07f2",
     "grade": true,
     "grade_id": "cell-afdeb02693cc4065",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_2(main_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3** <br> {points: 1}  \n",
    "\n",
    "Perform 5 fold cross-validation on `X_train` and `y_train` using the main pipeline `main_pipe`. Make sure to set `return_train_score=True` and save the result in a dataframe called `scores_df`. \n",
    "\n",
    "*Note: This could take 5 minutes. Remember how large our training data is.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c844bb3db03acae7cef3112de3409411",
     "grade": false,
     "grade_id": "cell-f59f426ce7ea817f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "scores_df = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21275f2bbb9908b5972e811b9dbce015",
     "grade": true,
     "grade_id": "cell-135f717050ddcca3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_3(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4** <br> {points: 2}\n",
    "\n",
    "What is the mean training and cross-validation scores? \n",
    "\n",
    "Save the mean training score in `mean_training_score` and the mean cross-validation score in the object named `cv_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3d44066eb7e87fc6142305a11fad604",
     "grade": false,
     "grade_id": "cell-bcfb2e25f0962f5a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mean_training_score = None\n",
    "cv_score = None \n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "print(mean_training_score, cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39f779e4b20c0ff7faa19775c6e399b0",
     "grade": true,
     "grade_id": "cell-43da91b0a4ccb4f4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check that the variable exists\n",
    "assert 'cv_score' in globals(\n",
    "), \"Please make sure that your solution is named 'cv_score'\"\n",
    "\n",
    "assert 'mean_training_score' in globals(\n",
    "), \"Please make sure that your solution is named 'mean_training_score'\"\n",
    "# This test has been intentionally hidden. It will be up to you to decide if your solution\n",
    "# is sufficiently good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.5** <br> {points: 1}\n",
    "\n",
    "Is the model overfitting or underfitting? \n",
    "\n",
    "A) Overfitting\n",
    "\n",
    "B) Underfitting\n",
    "\n",
    "C) Neither\n",
    "\n",
    "*Answer in the cell below using the uppercase letter associated with your answer. Place your answer between `\"\"`, assign the correct answer to an object called `answer3_05`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de8868ef110890ed7e8a47154bb98ebc",
     "grade": false,
     "grade_id": "cell-81bbca69985958ef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer3_05 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "answer3_05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08d301abc9b835102d9124b9222669dd",
     "grade": true,
     "grade_id": "cell-2a1d6ed12303efaf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_5(answer3_05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.6** <br> {points: 1}\n",
    "\n",
    "Which model performed better?\n",
    "\n",
    "A) `RandomForestClassifier`\n",
    "\n",
    "B) `DummyClassifier`\n",
    "\n",
    "C) Neither\n",
    "\n",
    "*Answer in the cell below using the uppercase letter associated with your answer. Place your answer between `\"\"`, assign the correct answer to an object called `answer3_06`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2632abe70454658e23a1bacc4e8594d2",
     "grade": false,
     "grade_id": "cell-4a863bf2d192b6ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer3_06 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "answer3_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96c46088e16257c786f2241663f5c750",
     "grade": true,
     "grade_id": "cell-4ca0895e35271e81",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_6(answer3_06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.7** <br> {points: 1}  \n",
    "Now that we have our pipelines and a model let's tune the hyperparameter `max_depth`. \n",
    "\n",
    "Sweep over the hyperparameters in `param_grid` using `RandomizedSearchCV` with a  `cv=5`, `n_iter=5` and setting `return_train_score=True`. Don't forget to set `random_state=77`.\n",
    "\n",
    "Save your grid search in an object named `depth_search`. \n",
    "\n",
    "You may also want to set `verbose=2` since it may take some time. \n",
    "\n",
    "Don't forget to fit `depth_search`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4bf2fbb314f3c7524ba8c3f9c0cede5",
     "grade": false,
     "grade_id": "cell-b687a71a1bb9232c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {\n",
    "    \"randomforestclassifier__max_depth\": range(1,151,10)\n",
    "}\n",
    "depth_search = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1cce7f09596b674ae3cd135e1ec00b3",
     "grade": true,
     "grade_id": "cell-75006cd515e05a64",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_7(depth_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.8** <br> {points: 1}  \n",
    "\n",
    "Obtain the results for cross validation from grid search using `depth_search.cv_results_`.\n",
    "\n",
    "Select the columns:\n",
    "\n",
    "- `mean_test_score`\n",
    "- `param_randomforestclassifier__max_depth`\n",
    "- `mean_fit_time`\n",
    "- `rank_test_score`\n",
    "\n",
    "Sort your values in ascending order of `rank_test_score`. \n",
    "\n",
    "Make sure to save it as a dataframe and display it. Save this as an object named `grid_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb9f2bdc7614d97f23f8eae7ca2eaa39",
     "grade": false,
     "grade_id": "cell-c73fc243949e9f49",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "grid_results = None\n",
    "\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7207b66c68dcd2085093e88605374a2d",
     "grade": true,
     "grade_id": "cell-eadfb6c58d1ab663",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_8(grid_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.9** <br> {points: 1} \n",
    "\n",
    "What is the best hyperparameter value for `n_estimators`? Save it in an object named `best_depth`. \n",
    "\n",
    "What was the corresponding validation score for it? Save this in an object named `best_depth_score`. \n",
    "\n",
    "*Hint: `.best_params_`  and `.best_score_` are helpful here.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0bb6de3b832f053040f2a228d12bfbfa",
     "grade": false,
     "grade_id": "cell-edd98dcf457de3e5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "best_depth = None \n",
    "\n",
    "best_depth_score = None \n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86e5757ef8f646c2df2998f3bcf53b5f",
     "grade": true,
     "grade_id": "cell-e67515c39ce2aa0a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_9(depth_search, best_depth, best_depth_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluating on the test set <a name=\"5\"></a>\n",
    "<hr>\n",
    "\n",
    "Now that we have a best performing model, it's time to assess our model on the set aside test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.1** <br> {points: 2} \n",
    "\n",
    "What is the training score of the best scoring model? Save the result in an object named `train_score`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c94c4eb0cf1d9acb5f5a28ec1407e5b",
     "grade": false,
     "grade_id": "cell-77ddb96a4f2f131c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76bcee2fa3fb3bfdd24c52e39f0fc411",
     "grade": true,
     "grade_id": "cell-323ba99aab8bcb16",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 'train_score' in globals(\n",
    "), \"Please make sure that your solution is named 'train_score'\"\n",
    "# This test has been intentionally hidden. It will be up to you to decide if your solution\n",
    "# is sufficiently good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.2** <br> {points: 1} \n",
    "\n",
    "\n",
    "What is the test score of the best model? \n",
    "\n",
    "Score the best model from `depth_search` on `X_test` and `y_test`. \n",
    "\n",
    "Save the result in an object named `test_score`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e06a21d2b503d055b6d9195f8f91641",
     "grade": false,
     "grade_id": "cell-2d926c965be49673",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cba3d96d5a10d10a484292721128f9c7",
     "grade": true,
     "grade_id": "cell-061c35980f6ba1ac",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_4_2(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's develop our own SMS spam filtering system using Kaggle's [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset) that was originally referenced from [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). \n",
    "\n",
    "We will use `CountVectorizer` to encode text messages and `SVC` for classification. \n",
    "\n",
    "**Sorry for the offensive language in some text messages; it's the reality of such platforms ðŸ˜”. If you are sensitive to such language try not to read the raw messages.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df = pd.read_csv(\"data/spam.csv\", encoding=\"latin-1\")\n",
    "sms_df = sms_df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "sms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.1** <br> {points: 1}  \n",
    "\n",
    "Split `sms_df` into train (80%) and test splits (20%) setting `random_state=123`. \n",
    "Name your objects `text_train_df` and `text_test_df`. \n",
    "Examine the first few rows of the train portion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f1f9ca32408423c2ee953bea0370a90",
     "grade": false,
     "grade_id": "cell-9f7221d877a99905",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "text_train_df, text_test_df = None, None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc69c84147d9f25369b80f9b16c11565",
     "grade": true,
     "grade_id": "cell-145c736bdec68746",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_5_1(text_train_df, text_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.2** <br> {points: 1}  \n",
    "\n",
    "Split both `text_train_df` and `text_test_df` into the target and feature columns. Here,  `target` is the target column (`y`) and `sms` is the column in your `X`. \n",
    "    \n",
    "Name your objects `X_text_train`, `y_text_train` and  `X_text_test` `y_text_test`.\n",
    "\n",
    "*Hint: Make sure that you are using single brackets (a Pandas Series) for your target (y) objects. The tests will not pass unless your y variables are of type Pandas Series. This can be done by selecting the column target with single square brackets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a2d94e6a1ad2fae9f0456077b67cc39",
     "grade": false,
     "grade_id": "cell-cd7f8f3b4425a0fb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_text_train = None\n",
    "y_text_train = None\n",
    "X_text_test = None\n",
    "y_text_test = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "721b2993f04fe810592c650e9df415ca",
     "grade": true,
     "grade_id": "cell-d885629e054764a8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_5_2(X_text_train, X_text_test, y_text_train, y_text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.3** <br> {points: 2}  \n",
    "\n",
    "Note that in case of text data, the usual EDA is not applicable. In this exercise will carry out some simple EDA to get a sense of the data.  \n",
    "\n",
    "What's the label distribution in the target column (How many `ham` and how many `spam` values do you have in the column `target`) in the training set? \n",
    "\n",
    "Save the result in an object named `target_freq`.\n",
    "\n",
    "The autograder is expecting an answer as a pandas series. \n",
    "\n",
    "*Hint: There is function that we use quite often that will give us the frequency of each category in a column.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce63218cef6f2f9cd8a64f52c3f06525",
     "grade": false,
     "grade_id": "cell-66e5b9b27ae415fd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "target_freq = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49ab5ddb825de9bc44c30600339f603b",
     "grade": true,
     "grade_id": "cell-bb2c970a1a708333",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 'target_freq' in globals(\n",
    "), \"Please make sure that your solution is named 'target_freq'\"\n",
    "# This test has been intentionally hidden. It will be up to you to decide if your solution\n",
    "# is sufficiently good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.4** <br> {points: 1} \n",
    "\n",
    "What's the average length in characters of text messages? Save the value to the nearest whole value in an object named `avg_text`. \n",
    "\n",
    "*Hint: `str.len()` may come in handy here.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55bb231d7eaa1cdf86e0ef9f19e4b447",
     "grade": false,
     "grade_id": "cell-d514abe705701cec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "avg_text = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d56171c69472a1c021cc2dca97eddeb",
     "grade": true,
     "grade_id": "cell-b240e742af836b18",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_5_4(avg_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.5** <br> {points: 1} \n",
    "\n",
    "Would you classify `sms` column as a categorical column? Does it make sense to carry out one-hot encoding on this column?\n",
    "\n",
    "A) It is a categorical column and I would carry out one-hot encoding on this column.\n",
    "\n",
    "B) It is a categorical column and I would **NOT** carry out one-hot encoding on this column.\n",
    "\n",
    "C) It is a free text column and I would carry out one-hot encoding on this column.\n",
    "\n",
    "D) It is a free text column and I would **NOT** carry out one-hot encoding on this column.\n",
    "\n",
    "*Answer in the cell below using the uppercase letter associated with your answer. Place your answer between `\"\"`, assign the correct answer to an object called `answer5_05`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98c9e79c7daa1d99cf72d4a37c9f7df8",
     "grade": false,
     "grade_id": "cell-c01dfe36a4f467d5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer5_05 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "answer5_05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f0299d675dadb5df54a2a41e3f00217",
     "grade": true,
     "grade_id": "cell-dfe834250d6b09da",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_5_5(answer5_05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.6** <br> {points: 0}  \n",
    "Import `CountVectorizer` from the appropriate library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b41bb19a64a9d9851b2a8c01078b87f",
     "grade": true,
     "grade_id": "cell-f333f108a2c75667",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.7** <br> {points: 1} \n",
    "\n",
    "Transform the training data using `CountVectorizer` with default parameters. Create an object named `vec`, fit it on `X_text_train` and `y_text_train` and transform `X_text_train`. \n",
    "\n",
    "Save the newly transformed `X_text_train` in an object named `transformed_X_train`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e43d364099fec2b0be6547164b82e37",
     "grade": false,
     "grade_id": "cell-ca5eec2f0c737712",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vec = None\n",
    "transformed_X_train = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e8853c96229fc943a795c67db7094af",
     "grade": true,
     "grade_id": "cell-73aed6b046404ae9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_5_7(transformed_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.8** <br> {points: 1} \n",
    "\n",
    "How many features have been created to represent each text message? \n",
    "\n",
    "Save the value in an object named `vocab_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dee70563cfff2cdc6bb9cb119a46675e",
     "grade": false,
     "grade_id": "cell-e442238d313db760",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7cd86ef9f58ad7820447f080273c200",
     "grade": true,
     "grade_id": "cell-72c5da0c2ffb0d8f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_5_8(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.9** <br> {points: 2} \n",
    "\n",
    "What does each feature represent and each feature value represent? \n",
    "\n",
    "A) A word in the corpus with the value representing the number of times the word occurs in the given text message.\n",
    "\n",
    "B) A text message in the corpus with the value representing the distance from the closest text in the corpus.\n",
    "\n",
    "C) An example in the corpus with the value representing the length of the text message.\n",
    "\n",
    "D) A sentence in the corpus with the value representing the number of times the sentence occurs in the given text message.\n",
    "\n",
    "*Answer in the cell below using the uppercase letter associated with your answer. Place your answer between `\"\"`, assign the correct answer to an object called `answer5_09`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca41355d95f4971a8ec20811dc97101a",
     "grade": false,
     "grade_id": "cell-17fce569088b4dde",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer5_09 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "answer5_09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "486a0f328eae1776cbe2d02aa283d201",
     "grade": true,
     "grade_id": "cell-76d7d64f99f7bd0e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 'answer5_09' in globals(\n",
    "), \"Please make sure that your solution is named 'answer5_09'\"\n",
    "# This test has been intentionally hidden. It will be up to you to decide if your solution\n",
    "# is sufficiently good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.10** <br> {points: 1} \n",
    "\n",
    "Build a pipeline named `dummy_pipe` for feature extraction using `CountVectorizer` with `binary=True` and `DummyClassifier` with strategy equal to `most_frequent`.\n",
    "\n",
    "Use `cross_validate()`setting `cv=5` with `dummy_pipe` and set `return_train_score=True` on `X_text_train` and `y_text_train` to obtain the train and test scores. \n",
    "\n",
    "Save this in a dataframe named `dummy_scores`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e555219e0e932efb5808d021f9295dc7",
     "grade": false,
     "grade_id": "cell-3f148854b5096bc9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_pipe = None\n",
    "dummy_scores = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b52996cb4e1d0aa6c3e1f4107e27f950",
     "grade": true,
     "grade_id": "cell-bb094f41a3d43e3d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_5_10(dummy_pipe, dummy_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.11** <br> {points: 1} \n",
    "\n",
    "What are the mean values of the columns in `dummy_scores`? Save this in an object named `dummy_scores_mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7f2749888ecc9aa57192d5a66496d1d",
     "grade": false,
     "grade_id": "cell-519fd24a924c6812",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_scores_mean = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27c67c818c4b3ca62e5e49741dfb23b4",
     "grade": true,
     "grade_id": "cell-d7b59c9f43453d49",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_5_11(dummy_scores_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.12** <br> {points: 1} \n",
    "\n",
    "Very often representing your free text feature values in a binary format works better in practice than the default one and so we are going with that. \n",
    "\n",
    "Now build a pipeline named `svc_pipe_binary` for feature extraction using `CountVectorizer` with `binary=True` and `SVC` with default hyperparameters. Make sure you are using `make_pipeline()` for this. \n",
    "\n",
    "Cross validate on `svc_pipe_binary` using `X_text_train` and `y_text_train` and setting `cv=5`  and `return_train_score=True`.  \n",
    "\n",
    "Save the results in a dataframe named `svc_scores`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "130e89cb2cb6bc51d25f7e41e1229d82",
     "grade": false,
     "grade_id": "cell-f8aa769efb4ddd07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9ca7b28765e05acd77c9f325144b217",
     "grade": true,
     "grade_id": "cell-ee7740c1ebaa2d8b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_5_12(svc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.13** <br> {points: 1} \n",
    "\n",
    "What are the mean values of the columns in `svc_scores`? Save this in an object named `svc_scores_mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9213ccc7e17e8849ec4a4ab9e83563ee",
     "grade": false,
     "grade_id": "cell-1de70ee7cd990ad9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "svc_scores_mean = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1abd6ce33c3245feb8c027a37c9626a",
     "grade": true,
     "grade_id": "cell-3ee2f16c5eff385a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_5_13(svc_scores_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.14** <br> {points: 1} \n",
    "\n",
    "Are you getting better results with `SVC` compared to `DummyClassifier`?\n",
    "\n",
    "A) I am getting better results with `SVC`.\n",
    "\n",
    "B) I am getting better results with `DummyClassifier`.\n",
    "\n",
    "\n",
    "*Answer in the cell below using the uppercase letter associated with your answer. Place your answer between `\"\"`, assign the correct answer to an object called `answer5_14`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf8bdeb49e72cbc85450b3d001fe096f",
     "grade": false,
     "grade_id": "cell-f870ead92831a546",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer5_14 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "answer5_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afa296be35dcb623101b4c60219dbf22",
     "grade": true,
     "grade_id": "cell-b91c1c3189925f5e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_5_14(answer5_14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributions\n",
    "- The Olympics Games DataSet - [Kaggle](https://www.kaggle.com/samruddhim/olympics-althlete-events-analysis)\n",
    "\n",
    "- The SMS Spam Collection Dataset - [Kaggle](https://www.kaggle.com/uciml/sms-spam-collection-dataset) and [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)\n",
    "\n",
    "    *Almeida, T.A., GÃƒÂ³mez Hidalgo, J.M., Yamakami, A. Contributions to the Study of SMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM Symposium on Document Engineering (DOCENG'11), Mountain View, CA, USA, 2011*\n",
    "\n",
    "\n",
    "- MDS DSCI 571 - Supervised Learning I - [MDS's GitHub website](https://github.com/UBC-MDS/DSCI_571_sup-learn-1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Submitting \n",
    "\n",
    "Before submitting your assignment please do the following:\n",
    "\n",
    "- Read through your solutions\n",
    "- **Restart your kernel and clear output and rerun your cells from top to bottom** \n",
    "- Makes sure that none of your code is broken \n",
    "- Verify that the tests from the questions you answered have obtained the output \"Success\"\n",
    "\n",
    "This is a simple way to make sure that you are submitting all the variables needed to mark the assignment. This method should help avoid losing marks due to changes in your environment.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
