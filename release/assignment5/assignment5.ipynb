{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Machine Learning  \n",
    "\n",
    "## Assignment 5: Preprocessing Numerical Features, Pipelines and Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't learn technical subjects without hands-on practice. The assignments are an important part of the course. To submit this assignment you will need to make sure that you save your Jupyter notebook. \n",
    "\n",
    "Below are the links of 2 videos that explain:\n",
    "\n",
    "1. [How to save your Jupyter notebook](https://youtu.be/0aoLgBoAUSA) and,       \n",
    "2. [How to answer a question in a Jupyter notebook assignment](https://youtu.be/7j0WKhI3W4s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Learning Goals:\n",
    "\n",
    "By the end of the module, students are expected to:\n",
    "\n",
    "- Identify when to implement feature transformations such as imputation and scaling.\n",
    "- Apply `sklearn.pipeline.Pipeline` to build a machine learning pipeline.\n",
    "- Use `sklearn` for applying numerical feature transformations on the data.\n",
    "- Discuss the golden rule in the context of feature transformations.\n",
    "- Carry out hyperparameter optimization using `sklearn`'s `GridSearchCV` and `RandomizedSearchCV`.\n",
    "- Explain overfitting on the validation set.\n",
    "\n",
    "\n",
    "This assignment covers [Module 5](https://ml-learn.mds.ubc.ca/en/module5) of the online course. You should complete this module before attempting this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any place you see `...`, you must fill in the function, variable, or data to complete the code. Substitute the `None` with your completed code and answers then proceed to run the cell!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the questions in this assignment will have hidden tests. This means that no feedback will be given as to the correctness of your solution. It will be left up to you to decide if your answer is sufficiently correct. These questions are worth 2 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2366229b01320e1e79c01a8af93e5cb6",
     "grade": false,
     "grade_id": "cell-5e92ac69d9a55154",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries needed for this lab\n",
    "from hashlib import sha1\n",
    "\n",
    "import altair as alt\n",
    "import graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    Normalizer,\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    normalize,\n",
    "    scale)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import test_assignment5 as t\n",
    "#alt.renderers.enable('mimetype')\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducing and Exploring the dataset <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "\n",
    "In this lab you will be working on a sample of [the adult census dataset](https://www.kaggle.com/uciml/adult-census-income#). We have made some modification to this data so that it's easier to work with. \n",
    "\n",
    "This is a classification dataset and the classification task is to predict whether income exceeds 50K per year or not based on the census data. You can find more information on the dataset and features [here](http://archive.ics.uci.edu/ml/datasets/Adult).\n",
    "\n",
    "\n",
    "*Note that many popular datasets have sex as a feature where the possible values are male and female. This representation reflects how the data were collected and is not meant to imply that, for example, gender is binary.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_df = pd.read_csv(\"data/income.csv\")\n",
    "census_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we will be looking at the numeric columns only and then we will look into the both the categorical and numeric columns in the following assignment after we've learned how to preprocess them in module 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_df = census_df.drop(columns=['education', 'occupation', 'relationship', 'race', 'native.country'])\n",
    "census_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1** <br> {points: 1}  \n",
    "\n",
    "In order to avoid violation of the golden rule, the first step before we do anything is splitting the data. \n",
    "\n",
    "Split the data into `train_df` (80%) and `test_df` (20%). Keep the target column (`income`) in the splits so that we can use it in EDA. Make sure to set `random_state=123` for grading purposes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "102fa8ae60fb3672b8ed15354f756905",
     "grade": false,
     "grade_id": "cell-987bbb5a53b6ea7e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = None, None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f985fad87ac1abdeb504dc1b6131a2e",
     "grade": true,
     "grade_id": "cell-47eb69ca1f5acf49",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_1(train_df,test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2** <br> {points: 1}  \n",
    "\n",
    "Let's examine our train_df column dtypes. \n",
    "\n",
    "Do you notice anything odd? Which column needs further investigation? \n",
    "\n",
    "*Answer in the cell below by place the column label in `\"\"` and assign it to an object called `answer1_2`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9905abe2c0e12a35476a456e7fcc1b43",
     "grade": false,
     "grade_id": "cell-c07cc62a7df9f3b4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer1_2 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "281dcf591a8c6e54b4e617c8f2e7c05f",
     "grade": true,
     "grade_id": "cell-74cf2cd6c3ec7c17",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_2(answer1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3** <br> {points: 1}  \n",
    "\n",
    "Take a look at the unique possible values in this column using [`.unique()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.unique.html) or `.value_counts()`. \n",
    "\n",
    "Which value is not numerical? Save this value as a string by placing it between `\"\"` and assigning it to an object called `answer1_3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0fbd7fa2a025f7f257db1e0491c2dbc",
     "grade": false,
     "grade_id": "cell-4d237158166928e6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer1_3 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3d53798e613a66df6b7d24e69fcd3eb",
     "grade": true,
     "grade_id": "cell-f9f43ffbfd7e4591",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_3(answer1_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.4** <br> {points: 1}  \n",
    "\n",
    "Looking at the previous question, these values were likely questions not answered by some people during the census.\n",
    "\n",
    "Usually `.describe()` or `.info()` methods would give you information on missing values. But here, they won't pick up this value as missing values as they are encoded as strings instead of an actual NaN in Python.\n",
    "\n",
    "Let's replace them with `np.NaN` before we carry out EDA. Name your new `train_df` `test_df` dataframes with the replaced values `train_df_nan` and `test_df_nan` respectively. If you do not do it, you'll encounter an error later on when you try to pass this data to a classifier. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0be7d1de0bc059301961006342a468a0",
     "grade": false,
     "grade_id": "cell-1eb90f5cf105e18a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d1ef4fd9b927746848c97475bc373c3",
     "grade": true,
     "grade_id": "cell-aa8b9ddba8d3b6ae",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_4(train_df_nan,test_df_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.5** <br> {points: 1}  \n",
    "\n",
    "Now, that we've replaced the string values with a numerical value (`NaN` is a float), transform this column to dtype `float64`. Save it as the same column name in the original `train_df_nan` and `test_df_nan` dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02d0a35f9266c4f71f4d8ca148ff39cb",
     "grade": false,
     "grade_id": "cell-9c539c621d5b5295",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b3cd838b981c01e1c69496a925c9a67",
     "grade": true,
     "grade_id": "cell-2718ec7a3438cb03",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_5(train_df_nan,test_df_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.6** <br> {points: 1}  \n",
    "\n",
    "Use `.describe()` to show summary statistics of each feature in the `train_df_nan` dataframe. Save this in an object named `train_stats`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd24eba73249f288030028dfa158817d",
     "grade": false,
     "grade_id": "cell-8277f6a846d48112",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_stats = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c421cad9abd1d233608b7304d084162b",
     "grade": true,
     "grade_id": "cell-2c95a3aa7c9fd0a6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_6(train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.7** <br> {points: 2}  \n",
    "\n",
    "What was highest capital loss someone reported? Save this in an object named `cap_loss_high`. \n",
    "\n",
    "What is the average number of years people reported of spending time on their education? Save this in an object named `edu_avg_yrs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a337189c86f8397108839cd44259f30",
     "grade": false,
     "grade_id": "cell-fa7ce9269333cbef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cap_loss_high = None\n",
    "edu_avg_yrs = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "414fc2f5398d2a9be877cd2c89e0adca",
     "grade": true,
     "grade_id": "cell-c16c1787379a8d77",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check that the variable exists\n",
    "assert 'cap_loss_high' in globals(\n",
    "), \"Please make sure that your solution is named 'cap_loss_high'\"\n",
    "\n",
    "# This test has been intentionally hidden. It will be up to you to decide if your solution\n",
    "# is sufficiently good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd6de57c8c316cb0f772c46badf6cab0",
     "grade": true,
     "grade_id": "cell-570ea3a343775d38",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_7_2(edu_avg_yrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.8** <br> {points: 1} \n",
    "\n",
    "We have provided you with some visualization of the features values plotted as histograms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(df,feature):\n",
    "    \"\"\"\n",
    "    plots a histogram of a decision trees feature\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature: str\n",
    "        the feature name\n",
    "    Returns\n",
    "    -------\n",
    "    altair.vegalite.v3.api.Chart\n",
    "        an Altair histogram \n",
    "    \"\"\"\n",
    "    histogram = alt.Chart(df).mark_bar(\n",
    "        opacity=0.7).encode(\n",
    "        alt.X(str(feature) + str(':O'), bin=alt.Bin(maxbins=50)),\n",
    "        alt.Y('count()', stack=None),\n",
    "        alt.Color('income:N')).properties(\n",
    "        title= str.title(feature))\n",
    "    return histogram\n",
    "\n",
    "feature_list = train_stats.columns[:3]\n",
    "figure_dict = dict()\n",
    "for feature in feature_list:\n",
    "    train_df_nan = train_df_nan.sort_values('income')\n",
    "    figure_dict.update({feature:plot_histogram(train_df_nan,feature)})\n",
    "figure_panel = alt.vconcat(*figure_dict.values())\n",
    "figure_panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these features, which seems the most relevant for the given prediction task? \n",
    "\n",
    "*Assign `answer1_8` the the column label name as an string. in a list and save it as an object named `answer1_8`*.\n",
    "*For example if you believe the answer is `age`, your answer would like this:*  \n",
    "\n",
    "`answer1_8 = 'age'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb57ed6581ed37441c691f5f47b195eb",
     "grade": false,
     "grade_id": "cell-7693dbd858b550f9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer1_8 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0a98e83ea8bf1c108c1651e8a983cd8",
     "grade": true,
     "grade_id": "cell-26c074ac56ad71cf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_8(answer1_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.9** <br> {points: 1} \n",
    "\n",
    "Let's now separate feature vectors from the targets.  Create `X_train`, `y_train`, `X_test`, `y_test` from `train_df_nan` and `test_df_nan`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b844e5fb4d6b8d248251790b7799a38",
     "grade": false,
     "grade_id": "cell-d7793ad84241aa5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train = None \n",
    "y_train = None \n",
    "X_test = None \n",
    "y_test = None \n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61c244428daaf93f23085e8b4a3aee23",
     "grade": true,
     "grade_id": "cell-f6fe7723338b55d5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_9(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.10** <br> {points: 1} \n",
    "\n",
    "At this point, if you train [`sklearn`'s `SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model on `X_train` and `y_train` would it work? \n",
    "\n",
    "A) Yes, it would train but we make not get meaningful results without scaling. \n",
    "\n",
    "B) Yes, it would train and it may give results that are descent enough. \n",
    "\n",
    "C) No, it can't train since we have not scaled yet. \n",
    "\n",
    "D) No, it can't train since we have not imputed yet.\n",
    "\n",
    "*Answer in the cell below using the uppercase letter associated with your answer. Place your answer between `\"\"`, assign the correct answer to an object called `answer1_10`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47fc4e1e11f9a9c557bb5f65175a11e2",
     "grade": false,
     "grade_id": "cell-fa93a14411cde339",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer1_10 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "answer1_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2d62bf183ac6467784c596fc2494320",
     "grade": true,
     "grade_id": "cell-8b8ca99d3b219084",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_10(answer1_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing - Imputation and Scaling without Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1** <br> {points: 1}  \n",
    "\n",
    "Before preprocessing our data, build a dummy classifier using `strategy=\"prior\"`. Carry out 5-fold cross validation on `X_train` and `y_train` using ` cross_validate()`. Don't forget to include the training_score. \n",
    "\n",
    "Save the results in a dataframe named `dummy_scores`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca393f12f4d843df642dd419b6e5a54b",
     "grade": false,
     "grade_id": "cell-f872893978cb3466",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_scores = None \n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4047dc40524050ad215cc2fdc8ba087",
     "grade": true,
     "grade_id": "cell-473fa195fcb18074",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_1(dummy_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2** <br> {points: 1}  \n",
    "\n",
    "Now impute missing values **without** using `sklearn.pipeline.Pipeline`.\n",
    "\n",
    "In this exercise you'll be imputing missing values **without using `scikit-learn` pipelines**. \n",
    "\n",
    "The goal here is two-fold. First, to understand what happens under the hood when you use `scikit-learn` `Pipelines`, and second, to convince yourself why it's a good idea to use pipelines.  \n",
    "\n",
    "For numeric features, use [`scikit-learn`'s `SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) to impute `NaN` values with `strategy=\"median\"`. Remember to apply the transformations on both the train and test splits.  \n",
    "\n",
    "Save your `SimpleImputer()` in an object named `imp`. Next transform `X_train` and `X_test` using `imp` and save the results in objects named `X_imp_train` and `X_imp_test` respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d1aa22e05b13745d0cf1a84cba11a09",
     "grade": false,
     "grade_id": "cell-2f98f5b9513f9351",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "imp = None\n",
    "X_imp_train = None\n",
    "X_imp_test = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28033672af54a1ae6d854b8e401b4c62",
     "grade": true,
     "grade_id": "cell-2111f91dd6cd8a9e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_2(imp,X_imp_train,X_imp_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3** <br> {points: 2}  \n",
    "\n",
    "When using the `SimpleImputer` transformer on the numeric columns, is there any problem with calling `fit_transform` on the test split? Why or why not? \n",
    "\n",
    "A) It is not problematic. \n",
    "\n",
    "B) It is problematic because it will imputing missing values with wrong values.\n",
    "\n",
    "C) It is problematic because we should never call fit on test data.\n",
    "\n",
    "D) It is problematic because it will throw an error in the code.\n",
    "\n",
    "*Answer in the cell below using the uppercase letter associated with your answer. Place your answer between `\"\"`, assign the correct answer to an object called `answer2_3`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bf1e735a1920308a8bae442a8705a88",
     "grade": false,
     "grade_id": "cell-7d5c901293db9f10",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer2_3 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "answer2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87367b5f8d1c464230a6d4a0f185386f",
     "grade": true,
     "grade_id": "cell-a547500f5f223f41",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check that the variable exists\n",
    "assert 'answer2_3' in globals(\n",
    "), \"Please make sure that your solution is named 'answer2_3'\"\n",
    "\n",
    "# This test has been intentionally hidden. It will be up to you to decide if your solution\n",
    "# is sufficiently good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4** <br> {points: 1}  \n",
    "\n",
    "Carry out cross validation using `cross_validate` on the preprocessed `X_imp_train` and `y_train` using the [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier) with default hyperparameters.\n",
    "Save your results in a dataframe named `scores`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a26b39676b2ccfbf1bedb3203eecff9e",
     "grade": false,
     "grade_id": "cell-d071ce27f726d79c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "scores = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd56527bde9788744b3670399c64d651",
     "grade": true,
     "grade_id": "cell-b99c7abe96c801fc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_4(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5** <br> {points: 1}  \n",
    "\n",
    "Are we violating the golden rule when we call `cross_validate` in 2.2? Why or why not? \n",
    "\n",
    "A) Yes, we are violating the golden rule since our test data is influencing our validation data. \n",
    "\n",
    "B) Yes, we are violating the Golden Rule because `cross_validate` is splitting the data after we already transformed it.\n",
    "\n",
    "C) No we are not violating the golden rule since our test data and training data are not influencing one another. \n",
    "\n",
    "D) No we are not violating the golden rule since our validation data is not being influenced by the training phase.\n",
    "\n",
    "*Answer in the cell below using the uppercase letter associated with your answer. Place your answer between `\"\"`, assign the correct answer to an object called `answer2_5`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "570903571f42bbdd83c8ca2b18892d6b",
     "grade": false,
     "grade_id": "cell-43fab68fb2f86a7c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer2_5 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "answer2_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5344d9f388eba433839d2554326b956c",
     "grade": true,
     "grade_id": "cell-792ad4ca623cf68e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_5(answer2_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing - Imputation and Scaling with Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1** <br> {points: 1}  \n",
    "\n",
    "In this question we are going to build a pipeline for multiple classifiers and append the results to a dictionary named `results_dict`.\n",
    "\n",
    "In this question we've written the most of the code for you. You will need to fill in the blank so that the code executes and produces a dataframe containing the statistics for each model. \n",
    "\n",
    "Make sure that the pipeline includes the transformers `SimpleImputer()` and `StandardScaler()`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {'Dummy': {'mean_train_accuracy': round(dummy_scores[\"train_score\"].mean(),4),\n",
    "                          'mean_validation_accuracy': round(dummy_scores[\"test_score\"].mean(),4),\n",
    "                          'mean_fit_time (s)': round(dummy_scores[\"fit_time\"].mean(),4),\n",
    "                          'mean_score_time (s)': round(dummy_scores[\"score_time\"].mean(),4)}}\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"Decision tree\": DecisionTreeClassifier(),\n",
    "    \"kNN\": KNeighborsClassifier(),\n",
    "    \"RBF SVM\": SVC(),\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(model_name, \":\")\n",
    "    \n",
    "    pipe = Pipeline(steps=[(\"imputer\", ...,\n",
    "                           (\"scaler\", ..., \n",
    "                           (\"classifier\", ...)])\n",
    "    \n",
    "    scores = ...(..., ..., ..., cv=5, return_train_score=True)\n",
    "    \n",
    "    results_dict[...] ={'mean_train_accuracy': scores[\"train_score\"].mean().round(4),\n",
    "                        'mean_validation_accuracy': scores[\"test_score\"].mean().round(4),\n",
    "                        'mean_fit_time (s)': scores[\"fit_time\"].mean().round(4),\n",
    "                        'mean_score_time (s)': scores[\"score_time\"].mean().round(4)\n",
    "                              }\n",
    "results_df = ...(results_dict).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d28c0ab731b8a34fe1775073c4894ad3",
     "grade": false,
     "grade_id": "cell-7a188f87caac1870",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f086581ae2659c271399b54a750b1222",
     "grade": true,
     "grade_id": "cell-479edf0b550cf435",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_1(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2** <br> {points: 1}  \n",
    "\n",
    "Which model produced the best score without hyperparameter tuning? \n",
    "Save you answer in an object named `highest_score`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "756eb1ade468b44c3c50243e410934f0",
     "grade": false,
     "grade_id": "cell-7fc96eefa05d4a1f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "highest_score = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "highest_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "263b46b80404c16b54d8d80605773532",
     "grade": true,
     "grade_id": "cell-9fb18dc5638b7e87",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_2(highest_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3** <br> {points: 1}  \n",
    "\n",
    "Which model appears to overfit the most? \n",
    "Save you answer in an object named `most_overfit`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52ad26c128cc4627ef42c98afd010eaf",
     "grade": false,
     "grade_id": "cell-0ea00051a9492762",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "most_overfit = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "most_overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8aff60d22249a705524b350c25599b4c",
     "grade": true,
     "grade_id": "cell-b6ab5de0c6a21238",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_3(most_overfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4** <br> {points: 2}  \n",
    "\n",
    "Which model takes the most time to fit? \n",
    "Save you answer in an object named `longest_fit`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e71e8a7554a77c4dabc27474cfe20a1",
     "grade": false,
     "grade_id": "cell-e8a6e481a676f174",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "longest_fit = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "longest_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fc405bdb9739b091489d569023f55cf",
     "grade": true,
     "grade_id": "cell-d6009a3eb2b7559c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check that the variable exists\n",
    "assert 'longest_fit' in globals(\n",
    "), \"Please make sure that your solution is named 'longest_fit'\"\n",
    "\n",
    "# This test has been intentionally hidden. It will be up to you to decide if your solution\n",
    "# is sufficiently good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hyperparameter Optimization\n",
    "\n",
    "Now that we have preprocessed features, and explored different models we are ready find optimal hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.1** <br> {points: 0}  \n",
    "Import `GridSearchCV` and `RandomizedSearchCV` from the appropriate library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb99ca5707ba337c7c22a74e4377f172",
     "grade": true,
     "grade_id": "cell-6d37db2739fbf9e1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.2** <br> {points: 1}  \n",
    "\n",
    "In this question you will tune the `n_neighbors` hyperparameter from the K-NN model. \n",
    "\n",
    "1. Create a pipeline with the steps `SimpleImputer(strategy=\"median\")`, `StandardScaler()` and  `KNeighborsClassifier()`in an object named `knn_pipe`.\n",
    "1. Sweep over the hyperparameters in `param_grid` in [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)(`param_grid` is given in the code below) and use 5-fold cross-validation. Similar to `cross_validate` you can pass `return_train_score=True` to your `GridSearchCV` object. Save this in an object named `k_search`. \n",
    "1. Fit `k_search` on `X_train` and `y_train`.\n",
    "\n",
    "*Hint: Setting `n_jobs=-1` should speed it up. This will take about 2 minutes to run.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e22bc4c2ce3fb3e70a319353286d1b8",
     "grade": false,
     "grade_id": "cell-8331499990be8264",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\"knn__n_neighbors\": np.arange(1, 50, 10)}\n",
    "knn_pipe = None\n",
    "k_search = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a42e8210133c00ccc1afeeb092d6e12",
     "grade": true,
     "grade_id": "cell-e131d5d6bdf4fe31",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_4_2(k_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.3** <br> {points: 4}  \n",
    "\n",
    "What is the best hyperparameter value for knn? Save it in an object named `best_k`. \n",
    "\n",
    "What was the corresponding validation score for it? Save this in an object named `best_k_score`. \n",
    "\n",
    "Does this model do better than without hyperparameter tuning `n_neighbors`?   <br> \n",
    "Answer as `True` or `False` in an object named `better_model`.\n",
    "\n",
    "\n",
    "<br> \n",
    "\n",
    "*Hint: `.best_params_`  and `.best_score_` are helpful here.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c155f700b3b9c6f9ef311b7ee55b0a88",
     "grade": false,
     "grade_id": "cell-136715ed0137c98d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "best_k = None \n",
    "\n",
    "best_k_score = None \n",
    "\n",
    "better_model = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d59672949bbb0b628233ee76ab5c5b5f",
     "grade": true,
     "grade_id": "cell-f1bb8b400800e55f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_4_3_1(best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43c966263d02f58a6c0f6119e2528718",
     "grade": true,
     "grade_id": "cell-36d4847079a6ba31",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_4_3_2(best_k_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9ced2a510a771343b24f6e0963bbe4c",
     "grade": true,
     "grade_id": "cell-d4d1fd43f2a38418",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check that the variable exists\n",
    "assert 'better_model' in globals(\n",
    "), \"Please make sure that your solution is named 'better_model'\"\n",
    "\n",
    "# This test has been intentionally hidden. It will be up to you to decide if your solution\n",
    "# is sufficiently good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.4** <br> {points: 1} \n",
    "\n",
    "Ok, let's step it up a notch and tune 2 hyperparameters at once and this time using [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html?highlight=randomizedsearchcv). \n",
    "\n",
    "This time let's find the optimal `gamma` and `C` values for a SVC model. \n",
    "\n",
    "1. Create a pipeline saving with the steps `SimpleImputer(strategy=\"median\")`, `StandardScaler()` and  `SVC()`in an object named `svc_pipe`.\n",
    "1. Sweep over the hyperparameters in `param_grid` in [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html?highlight=randomizedsearchcv) (again, `param_grid` is given in the code below) and use 5-fold cross-validation and specify `n_iter` to 5. Similar to `cross_validate` you can pass `return_train_score=True`  in `RandomizedSearchCV()`. Make sure to set `random_state=77` in `RandomizedSearchCV` or you will not pass the autograder. Save this in an object named `svc_search`. \n",
    "1. Fit `svc_search` on `X_train` and `y_train`.\n",
    "\n",
    "*Hint: Setting `n_jobs=-1` should speed it up but it may still take around 5 minutes to run. You may want to set `verbose=2` here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1959b3f1b19ed1a11c9d98ec13dca206",
     "grade": false,
     "grade_id": "cell-8def9c43a1f910a9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"svc__C\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"svc__gamma\": [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afc61172f3507486f41a4824eb8bac3e",
     "grade": true,
     "grade_id": "cell-5ca2686fc0bcc4d4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_4_4(svc_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.5** <br> {points: 2}  \n",
    "\n",
    "What is the best hyperparameter value for the svc model? Save it in an object named `best_svc`. \n",
    "\n",
    "What was the corresponding validation score for it? Save this in an object named `best_svc_score`. \n",
    "\n",
    "*Hint: `.best_params_`  and `.best_score_` are helpful here.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6daab871957888d1fcee0ca43001d838",
     "grade": false,
     "grade_id": "cell-6e652ef0e79365d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "best_svc = None \n",
    "\n",
    "best_svc_score = None \n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbaeb8ac8d6ec2b964e9595e4e3c876e",
     "grade": true,
     "grade_id": "cell-df3cfd0fc54f9ecc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_4_5_1(best_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2b52e08d8dfa22860d2add8215feae2",
     "grade": true,
     "grade_id": "cell-af3af88653c52b4f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_4_5_2(svc_search, best_svc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.6** <br> {points: 1}  \n",
    "\n",
    "***True or False***\n",
    "\n",
    "\n",
    "The `SVC` model without default hyperparameters scores higher.\n",
    "\n",
    "\n",
    "*Answer in the cell below by assigning `True` or `False` to an object called `answer4_6`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b0c5b4c7aa9239c4c2b74722bf0b28e",
     "grade": false,
     "grade_id": "cell-52d5993a07ddd33a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer4_6 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbac81334b6b48e3e81ea0f5834314d9",
     "grade": true,
     "grade_id": "cell-c97defa3c07e675f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_4_6(answer4_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluating on the test set <a name=\"5\"></a>\n",
    "<hr>\n",
    "\n",
    "Now that we have a best performing model, it's time to assess our model on the set aside test set. In this exercise you'll examine whether the results you obtained using cross-validation on the train set are consistent with the results on the test set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.1** <br> {points: 1} \n",
    "\n",
    "What is the training score of the best scoring model? Save the result in an object named `train_score`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfd8c0ae00c36627c527e884eaa76d6b",
     "grade": false,
     "grade_id": "cell-1e2cfa312996ba59",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fc5ea0ea76fc1e9c47411f960c68e39",
     "grade": true,
     "grade_id": "cell-4e0c9a4586b77bdb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_5_1(train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.2** <br> {points: 1} \n",
    "\n",
    "\n",
    "What is the test score of the best model? \n",
    "\n",
    "Score best model on `X_test` and `y_test`. \n",
    "\n",
    "Save the result in an object named `test_score`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7b3a921f2888df9dd0a8e59a942a1be",
     "grade": false,
     "grade_id": "cell-594fa80511decd0b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce529d1314a10cbb49439f52c63cc56d",
     "grade": true,
     "grade_id": "cell-25192f6ca274f4be",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_5_2(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Submitting \n",
    "\n",
    "Before submitting your assignment please do the following:\n",
    "\n",
    "- Read through your solutions\n",
    "- **Restart your kernel and clear output and rerun your cells from top to bottom** \n",
    "- Makes sure that none of your code is broken \n",
    "- Verify that the tests from the questions you answered have obtained the output \"Success\"\n",
    "\n",
    "This is a simple way to make sure that you are submitting all the variables needed to mark the assignment. This method should help avoid losing marks due to changes in your environment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributions\n",
    "- The adult census dataset - [Kaggle](https://www.kaggle.com/uciml/adult-census-income#)\n",
    "\n",
    "\n",
    "- MDS DSCI 571 - Supervised Learning I - [MDS's GitHub website](https://github.com/UBC-MDS/DSCI_571_sup-learn-1) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
